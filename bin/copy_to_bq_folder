#!/usr/bin/env python3

import click
import json
import os
from pathlib import Path, PurePath
import shutil

import yaml

@click.command()
@click.argument(
    'source_paths',
    type=click.Path(
        dir_okay=False,
        file_okay=True,
        writable=False,
        exists=True,
    ),
    required=True,
    nargs=-1,
)
def main(source_paths):
    for p in source_paths:
        process_path(p)


def process_path(source_path):
    source_path = Path(source_path)
    stem = '.'.join(source_path.name.split('.')[:2])
    source_json_path = source_path.parent / (stem + '.schema.json')
    source_bq_path = source_path.parent / (stem + '.bq')
    doctype_dir = source_path.parent
    namespace_dir = doctype_dir.parent
    schemas_dir = namespace_dir.parent
    root_dir = schemas_dir.parent
    bq_dir = root_dir / "bq"

    if source_json_path.exists() and source_bq_path.exists():
        print(f"Processing {source_bq_path} for copying to bq/")
    else:
        print(f"Not copying {source_path} into {bq_dir} due to missing schema")
        return

    with open(source_json_path, "r") as f:
        schema_contents = json.load(f)

    meta = schema_contents.get("mozPipelineMetadata", {})
    bq_dataset_family = meta.get("bq_dataset_family", None)
    bq_table = meta.get("bq_table", None)
    bq_project = "moz-fx-data-shared-prod"
    if bq_dataset_family and bq_table:
        dest_dataset_dir = bq_dir / bq_project / f"{bq_dataset_family}_stable"
        dest_table_dir = dest_dataset_dir / bq_table
        dest_table_dir.mkdir(parents=True, exist_ok=True)
        dest_bq_path = dest_table_dir / "schema.yaml"
        with source_bq_path.open("r") as f:
            bq_schema = json.load(f)
        print(f"Writing contents of {source_bq_path} to {dest_bq_path}")
        with dest_bq_path.open("w") as f:
            yaml.dump(bq_schema, f)
        #shutil.copy(source_bq_path, dest_bq_path)
        table_meta = {
            "source_schema": source_json_path,
            "bigquery": {
                "time_partitioning": {
                    "field": "submission_date",
                    "type": "day",
                    "require_partition_filter": True,
                },
                "clustering": {
                    "fields": ["normalized_channel", "sample_id"]
                },
            }
        }
        table_meta["mozPipelineMetadata"] = meta
        metadata_yaml_path = dest_table_dir / "metadata.yaml"
        with metadata_yaml_path.open('w') as f:
            yaml.dump(table_meta)
    else:
        print(f"Not copying {source_bq_path} into {bq_dir}")


if __name__ == "__main__":
    main()
