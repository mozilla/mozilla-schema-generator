#!/usr/bin/env python3

import click
import json
import os
from pathlib import Path, PurePath
import shutil


@click.command()
@click.argument(
    'source_paths',
    type=click.Path(
        dir_okay=False,
        file_okay=True,
        writable=False,
        exists=True,
    ),
    required=True,
    nargs=-1,
)
def main(source_paths):
    for p in source_paths:
        process_path(p)


def process_path(source_path):
    source_path = Path(source_path)
    stem = '.'.join(source_path.name.split('.')[:2])
    source_json_path = source_path.parent / (stem + '.schema.json')
    source_bq_path = source_path.parent / (stem + '.bq')
    doctype_dir = source_path.parent
    namespace_dir = doctype_dir.parent
    schemas_dir = namespace_dir.parent
    root_dir = schemas_dir.parent
    bq_dir = root_dir / "bq"

    if source_json_path.exists() and source_bq_path.exists():
        print(f"Processing {source_bq_path} for copying to bq/")
    else:
        print(f"Not copying {source_path} into {bq_dir} due to missing schema")
        return

    with open(source_json_path, "r") as f:
        schema_contents = json.load(f)

    meta = schema_contents.get("mozPipelineMetadata", {})
    bq_dataset_family = meta.get("bq_dataset_family", None)
    bq_table = meta.get("bq_table", None)
    if bq_dataset_family and bq_table:
        dest_dataset_dir = bq_dir / bq_dataset_family
        dest_dataset_dir.mkdir(parents=True, exist_ok=True)
        dest_bq_path = dest_dataset_dir / f"{bq_table}.bq"
        print(f"Copying {source_bq_path} to {dest_bq_path}")
        shutil.copy(source_bq_path, dest_bq_path)
        dest_json_path = dest_dataset_dir / f"{bq_table}.schema.json"
        if not dest_json_path.exists():
            relative_json_path = PurePath("../..") / source_json_path.relative_to(root_dir)
            dest_json_path.symlink_to(relative_json_path)
    else:
        print(f"Not copying {source_bq_path} into {bq_dir}")


if __name__ == "__main__":
    main()
